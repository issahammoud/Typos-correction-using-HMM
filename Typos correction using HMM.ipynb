{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First order HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train10.pkl\",\"rb\") as file:\n",
    "    train_10 = pk.load(file)\n",
    "\n",
    "with open(\"test10.pkl\",\"rb\") as file:\n",
    "    test_10 = pk.load(file)\n",
    "    \n",
    "with open(\"train20.pkl\",\"rb\") as file:\n",
    "    train_20 = pk.load(file)\n",
    "\n",
    "with open(\"test20.pkl\",\"rb\") as file:\n",
    "    test_20 = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_10) = 29057\n",
      "len(test_10)  = 1501\n",
      "len(train_20) = 27184\n",
      "len(test_20)  = 3374\n"
     ]
    }
   ],
   "source": [
    "print(\"len(train_10) = {}\\nlen(test_10)  = {}\\nlen(train_20) = {}\\nlen(test_20)  = {}\"\\\n",
    "      .format(len(train_10),len(test_10),len(train_20),len(test_20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \n",
    "    def __init__(self, train):\n",
    "        \n",
    "        flat = list(itertools.chain.from_iterable(train))\n",
    "        \n",
    "        self.state_list = np.unique([el[1] for el in flat])\n",
    "        self.N = len(self.state_list) # state_list length, just to verify that all alphabet appear\n",
    "        \n",
    "        self.obs_list = np.unique([el[0] for el in flat])\n",
    "        self.M = len(self.obs_list) # obs_list length, just to verify that all alphabet appear\n",
    "        \n",
    "        self.make_index(self.state_list, self.obs_list)\n",
    "        \n",
    "        self.init_state_proba = self.init_state_proba(train)\n",
    "        \n",
    "        self.trans_proba = self.trans_proba(train)\n",
    "        \n",
    "        self.emission_proba = self.emission_proba(train)\n",
    "        \n",
    "    def make_index(self, state_list, obs_list):\n",
    "        \n",
    "        self.state_index = {}\n",
    "        self.obs_index = {}\n",
    "        self.index_obs = {}\n",
    "        \n",
    "        for i, el in enumerate(state_list):\n",
    "            self.state_index[el] = i\n",
    "            \n",
    "        for i, el in enumerate(obs_list):\n",
    "            self.obs_index[el] = i\n",
    "            self.index_obs[i] = el\n",
    "            \n",
    "    def init_state_proba(self, train):\n",
    "        \n",
    "        pi = np.zeros(self.N)\n",
    "        \n",
    "        for el in train:\n",
    "            \n",
    "            pi[self.state_index[el[0][1]]] += 1\n",
    "            \n",
    "        return pi/np.sum(pi)\n",
    "    \n",
    "    def trans_proba(self, train):\n",
    "        \n",
    "        A = np.zeros((self.N,self.N))\n",
    "        \n",
    "        for el in train:\n",
    "            \n",
    "            for i in range(len(el)-1):\n",
    "                \n",
    "                A[self.state_index[el[i][1]]][self.state_index[el[i+1][1]]] += 1\n",
    "                    \n",
    "        for i in range(A.shape[0]):\n",
    "            A[i] /= np.sum(A[i])\n",
    "                \n",
    "        return A\n",
    "    \n",
    "    def emission_proba(self, train):\n",
    "        \n",
    "        B = np.zeros((self.N,self.M))\n",
    "        \n",
    "        flat = list(itertools.chain.from_iterable(train))\n",
    "        \n",
    "        count = Counter(flat)\n",
    "        \n",
    "        for x in self.state_list:\n",
    "            for y in self.obs_list:\n",
    "                B[self.state_index[x]][self.obs_index[y]] = count[(x,y)]\n",
    "                \n",
    "        for i in range(self.N):\n",
    "            B[i] /= np.sum(B[i])\n",
    "            \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(hmm,obs):\n",
    "    \n",
    "    N = hmm.N\n",
    "    \n",
    "    A = hmm.trans_proba\n",
    "    \n",
    "    B = hmm.emission_proba\n",
    "    \n",
    "    pi = hmm.init_state_proba\n",
    "    \n",
    "    state_index = hmm.state_index\n",
    "    \n",
    "    obs_index = hmm.obs_index\n",
    "    \n",
    "    index_obs = hmm.index_obs\n",
    "    \n",
    "    most_prob_seq = -np.ones((N,len(obs)))\n",
    "    \n",
    "    most_prob_seq[:,0] = np.arange(N)\n",
    "    \n",
    "    mu = B[:,obs_index[obs[0]]]*pi\n",
    "    \n",
    "    for k,i in enumerate(obs[1:]):\n",
    "        \n",
    "        tmp = np.zeros(N)\n",
    "        \n",
    "        for j in hmm.state_list:\n",
    "            \n",
    "            vect = B[state_index[j],obs_index[i]]*A[:,state_index[j]]*mu\n",
    "            \n",
    "            tmp[state_index[j]] = np.max(vect)\n",
    "            \n",
    "            most_prob_seq[state_index[j]][k+1] = np.argmax(vect)\n",
    "            \n",
    "        mu = tmp\n",
    "        \n",
    "    seq = []\n",
    "    \n",
    "    seq.append(index_obs[np.argmax(mu)])\n",
    "    \n",
    "    for i in range(len(obs)-1,0,-1):\n",
    "    \n",
    "        seq.append(index_obs[most_prob_seq[obs_index[seq[len(obs) - 1 - i]],i]])\n",
    "        \n",
    "        \n",
    "    return seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(true,test,pred):\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    neg = 0\n",
    "    \n",
    "    l1 = list(itertools.chain.from_iterable(true))\n",
    "    \n",
    "    l2 = list(itertools.chain.from_iterable(test))\n",
    "    \n",
    "    l3 = list(itertools.chain.from_iterable(pred))\n",
    "    \n",
    "    for i in range(len(l1)):\n",
    "        \n",
    "        if(l1[i] != l2[i] and l1[i] == l3[i]):\n",
    "            pos +=1\n",
    "            \n",
    "        if(l1[i] == l2[i] and l1[i] != l3[i]):\n",
    "            neg +=1\n",
    "            \n",
    "    return pos,neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true,pred):\n",
    "    \n",
    "    l1 = np.array(list(itertools.chain.from_iterable(true)))\n",
    "        \n",
    "    l2 = np.array(list(itertools.chain.from_iterable(pred)))\n",
    "    \n",
    "    return (l1 == l2).sum()/l1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with 10% error data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(train_10)\n",
    "test = []\n",
    "true = []\n",
    "for l in test_10:\n",
    "    test.append([el[0] for el in l])\n",
    "    true.append([el[1] for el in l])\n",
    "    \n",
    "pred = []\n",
    "\n",
    "for el in test:\n",
    "    pred.append(Viterbi(hmm,el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First order HMM made 79 mistakes and corrected 247 mistakes\n"
     ]
    }
   ],
   "source": [
    "pos,neg = eval(true,test,pred)\n",
    "\n",
    "print(\"First order HMM made {} mistakes and corrected {} mistakes\".format(neg,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy is about 92.12%\n"
     ]
    }
   ],
   "source": [
    "print(\"The overall accuracy is about {0:.2f}%\".format(accuracy(true,pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without doing anything, the accuracy is about 89.82%\n"
     ]
    }
   ],
   "source": [
    "print(\"Without doing anything, the accuracy is about {0:.2f}%\".format(accuracy(true,test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with 20% error data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(train_20)\n",
    "test = []\n",
    "true = []\n",
    "for l in test_20:\n",
    "    test.append([el[0] for el in l])\n",
    "    true.append([el[1] for el in l])\n",
    "    \n",
    "pred = []\n",
    "\n",
    "for el in test:\n",
    "    pred.append(Viterbi(hmm,el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First order HMM made 363 mistakes and corrected 1080 mistakes\n"
     ]
    }
   ],
   "source": [
    "pos,neg = eval(true,test,pred)\n",
    "\n",
    "print(\"First order HMM made {} mistakes and corrected {} mistakes\".format(neg,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy is about 84.89%\n",
      "Without doing anything, the accuracy is about 80.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"The overall accuracy is about {0:.2f}%\".format(accuracy(true,pred)*100))\n",
    "print(\"Without doing anything, the accuracy is about {0:.2f}%\".format(accuracy(true,test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
